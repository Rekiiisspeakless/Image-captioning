{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This notebook uses TensorFlow version 1.12.0\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = \"0\"\n",
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import _pickle as cPickle\n",
    "\n",
    "print(\"This notebook uses TensorFlow version {}\".format(tf.__version__))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total 26900 vocabularies\n"
     ]
    }
   ],
   "source": [
    "vocab = cPickle.load(open('dataset/vocab.pkl', 'rb'))\n",
    "print('total {} vocabularies'.format(len(vocab)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "count vocabulary occurances...\n",
      "3153 words appear >= 50 times\n"
     ]
    }
   ],
   "source": [
    "def count_vocab_occurance(vocab, df):\n",
    "    voc_cnt = {v: 0 for v in vocab}\n",
    "    for img_id, row in df.iterrows():\n",
    "        for w in row['caption'].split(' '):\n",
    "            voc_cnt[w] += 1\n",
    "    return voc_cnt\n",
    "\n",
    "\n",
    "df_train = pd.read_csv(os.path.join('dataset', 'train.csv'))\n",
    "\n",
    "print('count vocabulary occurances...')\n",
    "voc_cnt = count_vocab_occurance(vocab, df_train)\n",
    "\n",
    "# remove words appear < 50 times\n",
    "thrhd = 50\n",
    "x = np.array(list(voc_cnt.values()))\n",
    "print('{} words appear >= 50 times'.format(np.sum(x[(-x).argsort()] >= thrhd)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[transform captions into sequences of IDs]...\n"
     ]
    }
   ],
   "source": [
    "def build_voc_mapping(voc_cnt, thrhd):\n",
    "    \"\"\"\n",
    "    enc_map: voc --encode--> id\n",
    "    dec_map: id --decode--> voc\n",
    "    \"\"\"\n",
    "\n",
    "    def add(enc_map, dec_map, voc):\n",
    "        enc_map[voc] = len(dec_map)\n",
    "        dec_map[len(dec_map)] = voc\n",
    "        return enc_map, dec_map\n",
    "\n",
    "    # add <ST>, <ED>, <RARE>\n",
    "    enc_map, dec_map = {}, {}\n",
    "    for voc in ['<ST>', '<ED>', '<RARE>']:\n",
    "        enc_map, dec_map = add(enc_map, dec_map, voc)\n",
    "    for voc, cnt in voc_cnt.items():\n",
    "        if cnt < thrhd:  # rare words => <RARE>\n",
    "            enc_map[voc] = enc_map['<RARE>']\n",
    "        else:\n",
    "            enc_map, dec_map = add(enc_map, dec_map, voc)\n",
    "    return enc_map, dec_map\n",
    "\n",
    "\n",
    "enc_map, dec_map = build_voc_mapping(voc_cnt, thrhd)\n",
    "# save enc/decoding map to disk\n",
    "cPickle.dump(enc_map, open('dataset/enc_map.pkl', 'wb'))\n",
    "cPickle.dump(dec_map, open('dataset/dec_map.pkl', 'wb'))\n",
    "def caption_to_ids(enc_map, df):\n",
    "    img_ids, caps = [], []\n",
    "    for idx, row in df.iterrows():\n",
    "        icap = [enc_map[x] for x in row['caption'].split(' ')]\n",
    "        icap.insert(0, enc_map['<ST>'])\n",
    "        icap.append(enc_map['<ED>'])\n",
    "        img_ids.append(row['img_id'])\n",
    "        caps.append(icap)\n",
    "    return pd.DataFrame({\n",
    "              'img_id': img_ids,\n",
    "              'caption': caps\n",
    "            }).set_index(['img_id'])\n",
    "\n",
    "\n",
    "enc_map = cPickle.load(open('dataset/enc_map.pkl', 'rb'))\n",
    "print('[transform captions into sequences of IDs]...')\n",
    "df_proc = caption_to_ids(enc_map, df_train)\n",
    "df_proc.to_csv('dataset/train_enc_cap.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "decoding the encoded captions back...\n",
      "\n",
      "0: <ST> a group of three women sitting at a table sharing a cup of tea <ED>\n",
      "1: <ST> three women wearing hats at a table together <ED>\n",
      "2: <ST> three women with hats at a table having a tea party <ED>\n",
      "3: <ST> several woman dressed up with fancy hats at a tea party <ED>\n",
      "4: <ST> three women wearing large hats at a fancy tea event <ED>\n",
      "5: <ST> a twin door refrigerator in a kitchen next to cabinets <ED>\n",
      "6: <ST> a black refrigerator freezer sitting inside of a kitchen <ED>\n",
      "7: <ST> black refrigerator in messy kitchen of residential home <ED>\n"
     ]
    }
   ],
   "source": [
    "df_cap = pd.read_csv(\n",
    "    'dataset/train_enc_cap.csv')  # a dataframe - 'img_id', 'cpation'\n",
    "enc_map = cPickle.load(\n",
    "    open('dataset/enc_map.pkl', 'rb'))  # token => id\n",
    "dec_map = cPickle.load(\n",
    "    open('dataset/dec_map.pkl', 'rb'))  # id => token\n",
    "vocab_size = len(dec_map)\n",
    "\n",
    "\n",
    "def decode(dec_map, ids):\n",
    "    \"\"\"decode IDs back to origin caption string\"\"\"\n",
    "    return ' '.join([dec_map[x] for x in ids])\n",
    "\n",
    "\n",
    "print('decoding the encoded captions back...\\n')\n",
    "for idx, row in df_cap.iloc[:8].iterrows():\n",
    "    print('{}: {}'.format(idx, decode(dec_map, eval(row['caption']))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Images for training: 102739\n"
     ]
    }
   ],
   "source": [
    "img_train = cPickle.load(open('dataset/train_img256.pkl', 'rb'))\n",
    "# transform img_dict to dataframe\n",
    "img_train_df = pd.DataFrame(list(img_train.items()), columns=['img_id', 'img'])\n",
    "print('Images for training: {}'.format(img_train_df.shape[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_tfrecords(df_cap, img_df, filename, num_files=5):\n",
    "    ''' create tfrecords for dataset '''\n",
    "\n",
    "    def _float_feature(value):\n",
    "        return tf.train.Feature(\n",
    "            float_list=tf.train.FloatList(value=value))\n",
    "\n",
    "    def _int64_feature(value):\n",
    "        return tf.train.Feature(\n",
    "            int64_list=tf.train.Int64List(value=value))\n",
    "\n",
    "    num_records_per_file = img_df.shape[0] // num_files\n",
    "\n",
    "    total_count = 0\n",
    "\n",
    "    print(\"create training dataset....\")\n",
    "    for i in range(num_files):\n",
    "        # tfrecord writer: write record into files\n",
    "        count = 0\n",
    "        writer = tf.python_io.TFRecordWriter(\n",
    "            filename + '-' + str(i + 1) +'.tfrecords')\n",
    "        \n",
    "        # start point (inclusive)\n",
    "        st = i * num_records_per_file  \n",
    "        # end point (exclusive)\n",
    "        ed = (i + 1) * num_records_per_file if i != num_files - 1 else img_df.shape[0]  \n",
    "\n",
    "        for idx, row in img_df.iloc[st:ed].iterrows():\n",
    "        \n",
    "            # img representation in 256-d array format\n",
    "            img_representation = row['img']  \n",
    "\n",
    "            # each image has some captions describing it.\n",
    "            for _, inner_row in df_cap[df_cap['img_id'] == row['img_id']].iterrows():\n",
    "                # caption in different sequence length list format\n",
    "                caption = eval(inner_row['caption'])  \n",
    "\n",
    "                # construct 'example' object containing 'img', 'caption'\n",
    "                example = tf.train.Example(features=tf.train.Features(\n",
    "                    feature={\n",
    "                        'img': _float_feature(img_representation),\n",
    "                        'caption': _int64_feature(caption)\n",
    "                    }))\n",
    "\n",
    "                count += 1\n",
    "                writer.write(example.SerializeToString())\n",
    "        print(\"create {}-{}.tfrecords -- contains {} records\".format(\n",
    "                                    filename, str(i + 1), count))\n",
    "        total_count += count\n",
    "        writer.close()\n",
    "    print(\"Total records: {}\".format(total_count))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#uncomment next line to create tfrecords file\n",
    "#create_tfrecords(df_cap, img_train_df, 'dataset/tfrecords/train', 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of training records in all training file: 513969\n"
     ]
    }
   ],
   "source": [
    "import glob\n",
    "training_filenames = glob.glob('dataset/tfrecords/train-*')\n",
    "\n",
    "# get the number of records in training files\n",
    "def get_num_records(files):\n",
    "    count = 0\n",
    "    for fn in files:\n",
    "        for record in tf.python_io.tf_record_iterator(fn):\n",
    "            count += 1\n",
    "    return count\n",
    "\n",
    "num_train_records = get_num_records(training_filenames)\n",
    "print('Number of training records in all training file: {}'.format(\n",
    "    num_train_records))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def training_parser(record):\n",
    "    ''' parse record from .tfrecords file and create training record\n",
    "\n",
    "    :args \n",
    "      record - each record extracted from .tfrecords\n",
    "    :return\n",
    "      a dictionary contains {\n",
    "          'img': image array extracted from vgg16 (256-dim),\n",
    "          'input_seq': a list of word id\n",
    "                    which describes input caption sequence (Tensor),\n",
    "          'output_seq': a list of word id\n",
    "                    which describes output caption sequence (Tensor),\n",
    "          'mask': a list of one which describe\n",
    "                    the length of input caption sequence (Tensor)\n",
    "      }\n",
    "    '''\n",
    "\n",
    "    keys_to_features = {\n",
    "      \"img\": tf.FixedLenFeature([256], dtype=tf.float32),\n",
    "      \"caption\": tf.VarLenFeature(dtype=tf.int64)\n",
    "    }\n",
    "\n",
    "    # features contains - 'img', 'caption'\n",
    "    features = tf.parse_single_example(record, features=keys_to_features)\n",
    "\n",
    "    img = features['img']\n",
    "    caption = features['caption'].values\n",
    "    caption = tf.cast(caption, tf.int32)\n",
    "\n",
    "    # create input and output sequence for each training example\n",
    "    # e.g. caption :   [0 2 5 7 9 1]\n",
    "    #      input_seq:  [0 2 5 7 9]\n",
    "    #      output_seq: [2 5 7 9 1]\n",
    "    #      mask:       [1 1 1 1 1]\n",
    "    caption_len = tf.shape(caption)[0]\n",
    "    input_len = tf.expand_dims(tf.subtract(caption_len, 1), 0)\n",
    "\n",
    "    input_seq = tf.slice(caption, [0], input_len)\n",
    "    output_seq = tf.slice(caption, [1], input_len)\n",
    "    mask = tf.ones(input_len, dtype=tf.int32)\n",
    "\n",
    "    records = {\n",
    "      'img': img,\n",
    "      'input_seq': input_seq,\n",
    "      'output_seq': output_seq,\n",
    "      'mask': mask\n",
    "    }\n",
    "\n",
    "    return records"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tfrecord_iterator(filenames, batch_size, record_parser):\n",
    "    ''' create iterator to eat tfrecord dataset \n",
    "\n",
    "    :args\n",
    "        filenames     - a list of filenames (string)\n",
    "        batch_size    - batch size (positive int)\n",
    "        record_parser - a parser that read tfrecord\n",
    "                        and create example record (function)\n",
    "\n",
    "    :return \n",
    "        iterator      - an Iterator providing a way\n",
    "                        to extract elements from the created dataset.\n",
    "        output_types  - the output types of the created dataset.\n",
    "        output_shapes - the output shapes of the created dataset.\n",
    "    '''\n",
    "    dataset = tf.data.TFRecordDataset(filenames)\n",
    "    dataset = dataset.map(record_parser, num_parallel_calls=16)\n",
    "\n",
    "    # padded into equal length in each batch\n",
    "    dataset = dataset.padded_batch(\n",
    "      batch_size=batch_size,\n",
    "      padded_shapes={\n",
    "          'img': [None],\n",
    "          'input_seq': [None],\n",
    "          'output_seq': [None],\n",
    "          'mask': [None]\n",
    "      },\n",
    "      padding_values={\n",
    "          'img': 1.0,       # needless, for completeness\n",
    "          'input_seq': 1,   # padding input sequence in this batch\n",
    "          'output_seq': 1,  # padding output sequence in this batch\n",
    "          'mask': 0         # padding 0 means no words in this position\n",
    "      })  \n",
    "\n",
    "    dataset = dataset.repeat()             # repeat dataset infinitely\n",
    "    dataset = dataset.shuffle(3*batch_size)  # shuffle the dataset\n",
    "\n",
    "    iterator = dataset.make_initializable_iterator()\n",
    "    output_types = dataset.output_types\n",
    "    output_shapes = dataset.output_shapes\n",
    "\n",
    "    return iterator, output_types, output_shapes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_seq_embeddings(input_seq, vocab_size, word_embedding_size):\n",
    "    with tf.variable_scope('seq_embedding'), tf.device(\"/cpu:0\"):\n",
    "        embedding_matrix = tf.get_variable(\n",
    "            name='embedding_matrix',\n",
    "            shape=[vocab_size, word_embedding_size],\n",
    "            initializer=tf.random_uniform_initializer(minval=-1, maxval=1))\n",
    "        # [batch_size, padded_length, embedding_size]\n",
    "        seq_embeddings = tf.nn.embedding_lookup(embedding_matrix, input_seq)\n",
    "    return seq_embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_allow_growth_session():\n",
    "    config = tf.ConfigProto()\n",
    "    config.gpu_options.allow_growth = True\n",
    "    config.gpu_options.per_process_gpu_memory_fraction = 0.6\n",
    "    session = tf.Session(config=config)\n",
    "    return session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ImageCaptionModel(object):\n",
    "    ''' simple image caption model '''\n",
    "\n",
    "    def __init__(self, hparams, mode):\n",
    "        self.hps = hparams\n",
    "        self.mode = mode\n",
    "\n",
    "    def _build_inputs(self):\n",
    "        if self.mode == 'train':\n",
    "            self.filenames = tf.placeholder(\n",
    "                tf.string, shape=[None], name='filenames')\n",
    "            self.training_iterator, types, shapes = tfrecord_iterator(\n",
    "                self.filenames, self.hps.batch_size, training_parser)\n",
    "\n",
    "            self.handle = tf.placeholder(tf.string, shape=[], name='handle')\n",
    "            iterator = tf.data.Iterator.from_string_handle(\n",
    "                self.handle, types, shapes)\n",
    "            records = iterator.get_next()\n",
    "\n",
    "            image_embed = records['img']\n",
    "            image_embed.set_shape([None, self.hps.image_embedding_size])\n",
    "            input_seq = records['input_seq']\n",
    "            target_seq = records['output_seq']\n",
    "            input_mask = records['mask']\n",
    "\n",
    "        else:\n",
    "            image_embed = tf.placeholder(\n",
    "                tf.float32,\n",
    "                shape=[None, self.hps.image_embedding_size],\n",
    "                name='image_embed')\n",
    "            input_feed = tf.placeholder(\n",
    "                tf.int32, shape=[None], name='input_feed')\n",
    "\n",
    "            input_seq = tf.expand_dims(input_feed, axis=1)\n",
    "            # in inference step, only use image_embed\n",
    "            # and input_seq (the first start word)\n",
    "            target_seq = None\n",
    "            input_mask = None\n",
    "\n",
    "        self.image_embed = image_embed\n",
    "        self.input_seq = input_seq\n",
    "        self.target_seq = target_seq\n",
    "        self.input_mask = input_mask\n",
    "\n",
    "    def _build_seq_embeddings(self):\n",
    "        with tf.variable_scope('seq_embedding'), tf.device('/cpu:0'):\n",
    "            embedding_matrix = tf.get_variable(\n",
    "                name='embedding_matrix',\n",
    "                shape=[self.hps.vocab_size, self.hps.word_embedding_size],\n",
    "                initializer=tf.random_uniform_initializer(minval=-1, maxval=1))\n",
    "            # [batch_size, padded_length, embedding_size]\n",
    "            seq_embeddings = tf.nn.embedding_lookup(embedding_matrix,\n",
    "                                                    self.input_seq)\n",
    "\n",
    "        self.seq_embeddings = seq_embeddings\n",
    "\n",
    "    def _build_model(self):\n",
    "        # create rnn cell, you can choose different cell,\n",
    "        # even stack into multi-layer rnn\n",
    "        rnn_cell = tf.nn.rnn_cell.BasicLSTMCell(\n",
    "            num_units=self.hps.rnn_units, state_is_tuple=True)\n",
    "        rnn_cell = tf.contrib.rnn.DeviceWrapper(rnn_cell, \"/gpu:0\")\n",
    "        rnn_cell = tf.contrib.rnn.AttentionCellWrapper(rnn_cell, attn_length=self.hps.attn_length, state_is_tuple=True)\n",
    "        # when training, add dropout to regularize.\n",
    "        if self.mode == 'train':\n",
    "            rnn_cell = tf.nn.rnn_cell.DropoutWrapper(\n",
    "                rnn_cell,\n",
    "                input_keep_prob=self.hps.drop_keep_prob,\n",
    "                output_keep_prob=self.hps.drop_keep_prob)\n",
    "\n",
    "        # run rnn\n",
    "        with tf.variable_scope(\n",
    "                'rnn_scope',\n",
    "                initializer=tf.random_uniform_initializer(\n",
    "                    minval=-1, maxval=1)) as rnn_scope:\n",
    "\n",
    "            # feed the image embeddings to set the initial rnn state.\n",
    "            zero_state = rnn_cell.zero_state(\n",
    "                batch_size=tf.shape(self.image_embed)[0], dtype=tf.float32)\n",
    "            _, initial_state = rnn_cell(self.image_embed, zero_state)\n",
    "\n",
    "            rnn_scope.reuse_variables()\n",
    "\n",
    "            if self.mode == 'train':\n",
    "                sequence_length = tf.reduce_sum(self.input_mask, 1)\n",
    "                outputs, _ = tf.nn.dynamic_rnn(\n",
    "                    cell=rnn_cell,\n",
    "                    inputs=self.seq_embeddings,\n",
    "                    sequence_length=sequence_length,\n",
    "                    initial_state=initial_state,\n",
    "                    dtype=tf.float32,\n",
    "                    scope=rnn_scope)\n",
    "            else:\n",
    "                # in inference mode,\n",
    "                #  use concatenated states for convenient feeding and fetching.\n",
    "                initial_state = tf.concat(\n",
    "                    values=initial_state, axis=1, name='initial_state')\n",
    "\n",
    "                state_feed = tf.placeholder(\n",
    "                    tf.float32,\n",
    "                    shape=[None, sum(rnn_cell.state_size)],\n",
    "                    name='state_feed')\n",
    "                state_tuple = tf.split(\n",
    "                    value=state_feed, num_or_size_splits=2, axis=1)\n",
    "\n",
    "                # run a single rnn step\n",
    "                outputs, state = rnn_cell(\n",
    "                    inputs=tf.squeeze(self.seq_embeddings, axis=[1]),\n",
    "                    state=state_tuple)\n",
    "\n",
    "                # concatenate the resulting state.\n",
    "                final_state = tf.concat(\n",
    "                    values=state, axis=1, name='final_state')\n",
    "\n",
    "        # stack rnn output vertically\n",
    "        # [sequence_len * batch_size, rnn_output_size]\n",
    "        rnn_outputs = tf.reshape(outputs, [-1, rnn_cell.output_size])\n",
    "\n",
    "        # get logits after transforming from dense layer\n",
    "        with tf.variable_scope(\"logits\") as logits_scope:\n",
    "            rnn_out = {\n",
    "                'weights':\n",
    "                tf.Variable(\n",
    "                    tf.random_normal(\n",
    "                        shape=[self.hps.rnn_units, self.hps.vocab_size],\n",
    "                        mean=0.0,\n",
    "                        stddev=0.1,\n",
    "                        dtype=tf.float32)),\n",
    "                'bias':\n",
    "                tf.Variable(tf.zeros(shape=[self.hps.vocab_size]))\n",
    "            }\n",
    "\n",
    "            # logits [batch_size*seq_len, vocab_size]\n",
    "            logits = tf.add(\n",
    "                tf.matmul(rnn_outputs, rnn_out['weights']), rnn_out['bias'])\n",
    "\n",
    "        with tf.name_scope('optimize') as optimize_scope:\n",
    "            if self.mode == 'train':\n",
    "                targets = tf.reshape(self.target_seq,\n",
    "                                     [-1])  # flatten to 1-d tensor\n",
    "                indicator = tf.cast(\n",
    "                    tf.reshape(self.input_mask, [-1]), tf.float32)\n",
    "\n",
    "                # loss function\n",
    "                losses = tf.nn.sparse_softmax_cross_entropy_with_logits(\n",
    "                    labels=targets, logits=logits)\n",
    "                batch_loss = tf.div(\n",
    "                    tf.reduce_sum(tf.multiply(losses, indicator)),\n",
    "                    tf.reduce_sum(indicator),\n",
    "                    name='batch_loss')\n",
    "\n",
    "                # add some regularizer or tricks to train well\n",
    "                self.total_loss = batch_loss\n",
    "\n",
    "                # save checkpoint\n",
    "                self.global_step = tf.train.get_or_create_global_step()\n",
    "\n",
    "                # create optimizer\n",
    "                optimizer = tf.train.AdamOptimizer(learning_rate=self.hps.lr)\n",
    "                grads_and_vars = optimizer.compute_gradients(self.total_loss, tf.trainable_variables())\n",
    "                clipped_grads_and_vars = [(tf.clip_by_norm(gv[0], 1.0), gv[1]) for gv in grads_and_vars]\n",
    "                self.train_op = optimizer.apply_gradients(clipped_grads_and_vars, global_step=self.global_step)\n",
    "\n",
    "            else:\n",
    "                pred_softmax = tf.nn.softmax(logits, name='softmax')\n",
    "                prediction = tf.argmax(pred_softmax, axis=1, name='prediction')\n",
    "\n",
    "    def build(self):\n",
    "        self._build_inputs()\n",
    "        self._build_seq_embeddings()\n",
    "        self._build_model()\n",
    "\n",
    "    def train(self, training_filenames, num_train_records):\n",
    "        saver = tf.train.Saver()\n",
    "\n",
    "        with get_allow_growth_session() as sess:\n",
    "            ckpt = tf.train.get_checkpoint_state(self.hps.ckpt_dir)\n",
    "            if ckpt and ckpt.model_checkpoint_path:\n",
    "                # if checkpoint exists\n",
    "                saver.restore(sess, ckpt.model_checkpoint_path)\n",
    "                # assume the name of checkpoint is like '.../model.ckpt-1000'\n",
    "                gs = int(\n",
    "                    ckpt.model_checkpoint_path.split('/')[-1].split('-')[-1])\n",
    "                sess.run(tf.assign(self.global_step, gs))\n",
    "            else:\n",
    "                # no checkpoint\n",
    "                sess.run(tf.global_variables_initializer())\n",
    "\n",
    "            training_handle = sess.run(self.training_iterator.string_handle())\n",
    "            sess.run(\n",
    "                self.training_iterator.initializer,\n",
    "                feed_dict={self.filenames: training_filenames})\n",
    "\n",
    "            num_batch_per_epoch_train = num_train_records // self.hps.batch_size\n",
    "\n",
    "            loss = []\n",
    "            for epoch in range(self.hps.training_epochs):\n",
    "                _loss = []\n",
    "                for i in range(num_batch_per_epoch_train):\n",
    "                    train_loss_batch, _ = sess.run(\n",
    "                        [self.total_loss, self.train_op],\n",
    "                        feed_dict={self.handle: training_handle})\n",
    "                    _loss.append(train_loss_batch)\n",
    "                    if (i % 1000 == 0):\n",
    "                        print(\"minibatch training loss: {:.4f}\".format(\n",
    "                            train_loss_batch))\n",
    "                loss_this_epoch = np.sum(_loss)\n",
    "                gs = self.global_step.eval()\n",
    "                print('Epoch {:2d} - train loss: {:.4f}'.format(\n",
    "                    int(gs / num_batch_per_epoch_train), loss_this_epoch))\n",
    "                loss.append(loss_this_epoch)\n",
    "                saver.save(\n",
    "                    sess, self.hps.ckpt_dir + 'model.ckpt', global_step=gs)\n",
    "                print(\"save checkpoint in {}\".format(self.hps.ckpt_dir +\n",
    "                                                     'model.ckpt-' + str(gs)))\n",
    "\n",
    "            print('Done')\n",
    "\n",
    "    def beam_search(self, sess, rnn_state, prev_word, log_beam_prob, beam_size=3):\n",
    "        probs, next_state = sess.run(\n",
    "            fetches=['optimize/softmax:0', 'rnn_scope/final_state:0'],\n",
    "            feed_dict={\n",
    "                'input_feed:0': [prev_word],\n",
    "                'rnn_scope/state_feed:0': rnn_state\n",
    "            })\n",
    "        probs = probs[0]\n",
    "        probs_logsum = np.log(probs + 0.00001) + log_beam_prob\n",
    "        indices = np.argsort(probs_logsum)[::-1][0:beam_size]\n",
    "        best_probs = []\n",
    "        for idx in indices:\n",
    "            best_probs.append(probs_logsum[idx])\n",
    "        #  best_probs, indices = sess.run(tf.nn.top_k(probs_logsum, k=beam_size))\n",
    "        #  best_probs, indices = best_probs[0], indices[0]\n",
    "        next_beam_probs = []\n",
    "        next_words = []\n",
    "        for i in range(beam_size):\n",
    "            next_beam_probs.append(best_probs[i])\n",
    "            next_words.append(indices[i])\n",
    "        return next_state, next_words, next_beam_probs\n",
    "\n",
    "    def inference(self, sess, img_embed, enc_map, dec_map):\n",
    "        # get <start> and <end> word id\n",
    "        st, ed = enc_map['<ST>'], enc_map['<ED>']\n",
    "\n",
    "        caption_id = []\n",
    "        # feed into input_feed\n",
    "        start_word_feed = [st]\n",
    "\n",
    "        # feed image_embed into initial state\n",
    "        initial_state = sess.run(\n",
    "                fetches='rnn_scope/initial_state:0',\n",
    "                feed_dict={'image_embed:0': img_embed})\n",
    "\n",
    "        # get the first word and its state\n",
    "        nxt_word, this_state = sess.run(\n",
    "                fetches=['optimize/prediction:0', 'rnn_scope/final_state:0'],\n",
    "                feed_dict={\n",
    "                        'input_feed:0': start_word_feed,\n",
    "                        'rnn_scope/state_feed:0': initial_state\n",
    "                })\n",
    "\n",
    "        caption_id.append(int(nxt_word))\n",
    "\n",
    "        for i in range(self.hps.max_caption_len - 1):\n",
    "            nxt_word, this_state = sess.run(\n",
    "                    fetches=['optimize/prediction:0', 'rnn_scope/final_state:0'],\n",
    "                    feed_dict={\n",
    "                            'input_feed:0': nxt_word,\n",
    "                            'rnn_scope/state_feed:0': this_state\n",
    "                    })\n",
    "            caption_id.append(int(nxt_word))\n",
    "\n",
    "        caption = [\n",
    "                dec_map[x]\n",
    "                for x in caption_id[:None\n",
    "                    if ed not in caption_id else caption_id.index(ed)]\n",
    "        ]\n",
    "\n",
    "        return ' '.join(caption)\n",
    "\n",
    "    def beam_inference(self, sess, img_embed, enc_map, dec_map):\n",
    "        # get <start> and <end> word id\n",
    "        st, ed = enc_map['<ST>'], enc_map['<ED>']\n",
    "\n",
    "        # feed image_embed into initial state\n",
    "        initial_state = sess.run(\n",
    "            fetches='rnn_scope/initial_state:0',\n",
    "            feed_dict={'image_embed:0': img_embed})\n",
    "\n",
    "        # feed into input_feed\n",
    "        start_word_feed = st\n",
    "        # beam search\n",
    "        beam_size = 3\n",
    "\n",
    "        state, words, probs = self.beam_search(sess, initial_state, start_word_feed, [0], beam_size)\n",
    "        states = [state for i in range(beam_size)]\n",
    "\n",
    "        captions = [[] for i in range(beam_size)]\n",
    "        for i in range(beam_size):\n",
    "            captions[i].append(words[i])\n",
    "\n",
    "        for i in range(self.hps.max_caption_len - 1):\n",
    "            all_beam_states = []\n",
    "            all_beam_words = []\n",
    "            all_beam_probs = []\n",
    "            for j in range(beam_size):\n",
    "                nstate, nwords, nprobs = self.beam_search(sess, states[j], words[j], probs[j], beam_size)\n",
    "                for _ in range(beam_size):\n",
    "                    all_beam_states.append(nstate)\n",
    "                all_beam_words.extend(nwords)\n",
    "                all_beam_probs.extend(nprobs)\n",
    "            indices = (np.argsort(all_beam_probs)[::-1])[0:beam_size]\n",
    "            new_captions = [[] for i in range(beam_size)]\n",
    "            for j, index in enumerate(indices):\n",
    "                cap_id = index // beam_size\n",
    "                new_captions[j].extend(captions[cap_id])\n",
    "                new_captions[j].append(all_beam_words[index])\n",
    "                states[j] = all_beam_states[index]\n",
    "                words[j] = all_beam_words[index]\n",
    "                probs[j] = all_beam_probs[index]\n",
    "            captions = new_captions\n",
    "\n",
    "        caption_sentences = []\n",
    "        for caption in captions:\n",
    "            word_caption = [dec_map[x]\n",
    "                            for x in caption[:None if ed not in caption else\n",
    "                                caption.index(ed)]]\n",
    "            caption_sentences.append(' '.join(word_caption))\n",
    "\n",
    "        return caption_sentences[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_hparams():\n",
    "    hparams = tf.contrib.training.HParams(\n",
    "        vocab_size=vocab_size,\n",
    "        batch_size=128,\n",
    "        rnn_units=256,\n",
    "        image_embedding_size=256,\n",
    "        word_embedding_size=256,\n",
    "        drop_keep_prob=0.9,\n",
    "        lr=1e-3,\n",
    "        attn_length = 150,\n",
    "        training_epochs=50,\n",
    "        max_caption_len=15,\n",
    "        ckpt_dir='model_ckpt/')\n",
    "    return hparams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get hperparameters\n",
    "hparams = get_hparams()\n",
    "# create model\n",
    "tf.reset_default_graph()\n",
    "model = ImageCaptionModel(hparams,  mode='train' )\n",
    "model.build()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-12-02 21:10:25.134177: Start training.\n",
      "minibatch training loss: 5427.5562\n",
      "minibatch training loss: 1601.6249\n",
      "minibatch training loss: 442.8813\n",
      "minibatch training loss: 446.1489\n",
      "minibatch training loss: 250.1176\n",
      "Epoch  1 - train loss: 7273030.0000\n",
      "save checkpoint in model_ckpt/model.ckpt-4015\n",
      "minibatch training loss: 649.4360\n",
      "minibatch training loss: 287.0471\n",
      "minibatch training loss: 59.2003\n",
      "minibatch training loss: 607.4108\n",
      "minibatch training loss: 991.0073\n",
      "Epoch  2 - train loss: 8897434.0000\n",
      "save checkpoint in model_ckpt/model.ckpt-8030\n",
      "minibatch training loss: 11185.9932\n",
      "minibatch training loss: 7133.3979\n",
      "minibatch training loss: 6680.7837\n",
      "minibatch training loss: 570.2433\n",
      "minibatch training loss: 293.8163\n",
      "Epoch  3 - train loss: 18301958.0000\n",
      "save checkpoint in model_ckpt/model.ckpt-12045\n",
      "minibatch training loss: 663.4269\n",
      "minibatch training loss: 339.1167\n",
      "minibatch training loss: 147.4665\n",
      "minibatch training loss: 131.7395\n",
      "minibatch training loss: 213.2328\n",
      "Epoch  4 - train loss: 1637647.0000\n",
      "save checkpoint in model_ckpt/model.ckpt-16060\n",
      "minibatch training loss: 111.4477\n",
      "minibatch training loss: 485.6734\n",
      "minibatch training loss: 85.2119\n",
      "minibatch training loss: 196.1731\n",
      "minibatch training loss: 179.4688\n",
      "Epoch  5 - train loss: 2668422.0000\n",
      "save checkpoint in model_ckpt/model.ckpt-20075\n",
      "minibatch training loss: 725.7429\n",
      "minibatch training loss: 464.7032\n",
      "minibatch training loss: 120.0564\n",
      "minibatch training loss: 261.2780\n",
      "minibatch training loss: 79.9225\n",
      "Epoch  6 - train loss: 2154777.5000\n",
      "save checkpoint in model_ckpt/model.ckpt-24090\n",
      "minibatch training loss: 67.9861\n",
      "minibatch training loss: 85.9305\n",
      "minibatch training loss: 67.3161\n",
      "minibatch training loss: 113.8392\n",
      "minibatch training loss: 113.9297\n",
      "Epoch  7 - train loss: 1299468.0000\n",
      "save checkpoint in model_ckpt/model.ckpt-28105\n",
      "minibatch training loss: 85.7705\n",
      "minibatch training loss: 107.6942\n",
      "minibatch training loss: 471.7246\n",
      "minibatch training loss: 144.1563\n",
      "minibatch training loss: 498.1796\n",
      "Epoch  8 - train loss: 2807659.5000\n",
      "save checkpoint in model_ckpt/model.ckpt-32120\n",
      "minibatch training loss: 1621.6395\n",
      "minibatch training loss: 707.0287\n",
      "minibatch training loss: 4881.8828\n",
      "minibatch training loss: 1865.6105\n",
      "minibatch training loss: 257.6634\n",
      "Epoch  9 - train loss: 7352422.5000\n",
      "save checkpoint in model_ckpt/model.ckpt-36135\n",
      "minibatch training loss: 2907.0032\n",
      "minibatch training loss: 242.3532\n",
      "minibatch training loss: 477.3862\n",
      "minibatch training loss: 761.9545\n",
      "minibatch training loss: 2324.3850\n",
      "Epoch 10 - train loss: 8204561.0000\n",
      "save checkpoint in model_ckpt/model.ckpt-40150\n",
      "minibatch training loss: 666.8022\n",
      "minibatch training loss: 1859.8296\n",
      "minibatch training loss: 2179.8716\n",
      "minibatch training loss: 1096.8792\n",
      "minibatch training loss: 7685.4858\n",
      "Epoch 11 - train loss: 21696358.0000\n",
      "save checkpoint in model_ckpt/model.ckpt-44165\n",
      "minibatch training loss: 448.0284\n",
      "minibatch training loss: 3275.0339\n",
      "minibatch training loss: 8058.8848\n",
      "minibatch training loss: 1424.7812\n",
      "minibatch training loss: 2170.6604\n",
      "Epoch 12 - train loss: 26825988.0000\n",
      "save checkpoint in model_ckpt/model.ckpt-48180\n",
      "minibatch training loss: 2116.3772\n",
      "minibatch training loss: 13844.7109\n",
      "minibatch training loss: 564.6008\n",
      "minibatch training loss: 457.2914\n",
      "minibatch training loss: 265.5124\n",
      "Epoch 13 - train loss: 7280167.5000\n",
      "save checkpoint in model_ckpt/model.ckpt-52195\n",
      "minibatch training loss: 586.7891\n",
      "minibatch training loss: 896.1235\n",
      "minibatch training loss: 81.4660\n",
      "minibatch training loss: 89.8112\n",
      "minibatch training loss: 1137.1891\n",
      "Epoch 14 - train loss: 2286758.2500\n",
      "save checkpoint in model_ckpt/model.ckpt-56210\n",
      "minibatch training loss: 70.2921\n",
      "minibatch training loss: 500.4776\n",
      "minibatch training loss: 958.2783\n",
      "minibatch training loss: 115.2234\n",
      "minibatch training loss: 43.1182\n",
      "Epoch 15 - train loss: 1287828.0000\n",
      "save checkpoint in model_ckpt/model.ckpt-60225\n",
      "minibatch training loss: 164.2435\n",
      "minibatch training loss: 25.5499\n",
      "minibatch training loss: 226.2800\n",
      "minibatch training loss: 31.5457\n",
      "minibatch training loss: 28.9004\n",
      "Epoch 16 - train loss: 780471.8750\n",
      "save checkpoint in model_ckpt/model.ckpt-64240\n",
      "minibatch training loss: 28.2917\n",
      "minibatch training loss: 36.2132\n",
      "minibatch training loss: 19.9619\n",
      "minibatch training loss: 18.3047\n",
      "minibatch training loss: 14.0680\n",
      "Epoch 17 - train loss: 177413.6406\n",
      "save checkpoint in model_ckpt/model.ckpt-68255\n",
      "minibatch training loss: 16.5029\n",
      "minibatch training loss: 14.7253\n",
      "minibatch training loss: 18.9863\n",
      "minibatch training loss: 12.4633\n",
      "minibatch training loss: 19.1669\n",
      "Epoch 18 - train loss: 79753.8672\n",
      "save checkpoint in model_ckpt/model.ckpt-72270\n",
      "minibatch training loss: 18.2746\n",
      "minibatch training loss: 34.3918\n",
      "minibatch training loss: 15.3012\n",
      "minibatch training loss: 11.5952\n",
      "minibatch training loss: 23.4166\n",
      "Epoch 19 - train loss: 94367.1562\n",
      "save checkpoint in model_ckpt/model.ckpt-76285\n",
      "minibatch training loss: 13.7150\n",
      "minibatch training loss: 9.8603\n",
      "minibatch training loss: 9.2557\n",
      "minibatch training loss: 11.5984\n",
      "minibatch training loss: 9.5561\n",
      "Epoch 20 - train loss: 51454.6641\n",
      "save checkpoint in model_ckpt/model.ckpt-80300\n",
      "minibatch training loss: 9.4624\n",
      "minibatch training loss: 9.1111\n",
      "minibatch training loss: 9.2893\n",
      "minibatch training loss: 24.8034\n",
      "minibatch training loss: 22.6066\n",
      "Epoch 21 - train loss: 59144.5742\n",
      "save checkpoint in model_ckpt/model.ckpt-84315\n",
      "minibatch training loss: 25.7316\n",
      "minibatch training loss: 13.4265\n",
      "minibatch training loss: 67.6575\n",
      "minibatch training loss: 145.5714\n",
      "minibatch training loss: 139.7840\n",
      "Epoch 22 - train loss: 383958.3750\n",
      "save checkpoint in model_ckpt/model.ckpt-88330\n",
      "minibatch training loss: 192.3899\n",
      "minibatch training loss: 97.2087\n",
      "minibatch training loss: 165.9221\n",
      "minibatch training loss: 25.1887\n",
      "minibatch training loss: 20.3565\n",
      "Epoch 23 - train loss: 467673.5000\n",
      "save checkpoint in model_ckpt/model.ckpt-92345\n",
      "minibatch training loss: 23.6944\n",
      "minibatch training loss: 20.9864\n",
      "minibatch training loss: 23.1942\n",
      "minibatch training loss: 24.5608\n",
      "minibatch training loss: 54.8732\n",
      "Epoch 24 - train loss: 219011.6719\n",
      "save checkpoint in model_ckpt/model.ckpt-96360\n",
      "minibatch training loss: 40.8523\n",
      "minibatch training loss: 36.8188\n",
      "minibatch training loss: 24.0546\n",
      "minibatch training loss: 29.9863\n",
      "minibatch training loss: 256.4314\n",
      "Epoch 25 - train loss: 630179.2500\n",
      "save checkpoint in model_ckpt/model.ckpt-100375\n",
      "minibatch training loss: 199.7032\n",
      "minibatch training loss: 37.9081\n",
      "minibatch training loss: 38.6624\n",
      "minibatch training loss: 70.9891\n",
      "minibatch training loss: 30.9896\n",
      "Epoch 26 - train loss: 610916.3125\n",
      "save checkpoint in model_ckpt/model.ckpt-104390\n",
      "minibatch training loss: 35.5878\n",
      "minibatch training loss: 170.3877\n",
      "minibatch training loss: 95.0783\n",
      "minibatch training loss: 131.1411\n",
      "minibatch training loss: 385.8225\n",
      "Epoch 27 - train loss: 1616518.7500\n",
      "save checkpoint in model_ckpt/model.ckpt-108405\n",
      "minibatch training loss: 130.4217\n",
      "minibatch training loss: 123.6395\n",
      "minibatch training loss: 49.3174\n",
      "minibatch training loss: 50.3754\n",
      "minibatch training loss: 348.6925\n",
      "Epoch 28 - train loss: 1191473.0000\n",
      "save checkpoint in model_ckpt/model.ckpt-112420\n",
      "minibatch training loss: 35.4942\n",
      "minibatch training loss: 22.0343\n",
      "minibatch training loss: 146.7637\n",
      "minibatch training loss: 76.6034\n",
      "minibatch training loss: 50.6363\n",
      "Epoch 29 - train loss: 154251.4531\n",
      "save checkpoint in model_ckpt/model.ckpt-116435\n",
      "minibatch training loss: 36.8164\n",
      "minibatch training loss: 14.3835\n",
      "minibatch training loss: 17.3073\n",
      "minibatch training loss: 29.6573\n",
      "minibatch training loss: 17.3975\n",
      "Epoch 30 - train loss: 89437.2188\n",
      "save checkpoint in model_ckpt/model.ckpt-120450\n",
      "minibatch training loss: 15.0246\n",
      "minibatch training loss: 14.5151\n",
      "minibatch training loss: 17.5722\n",
      "minibatch training loss: 18.3997\n",
      "minibatch training loss: 16.8153\n",
      "Epoch 31 - train loss: 118777.7266\n",
      "save checkpoint in model_ckpt/model.ckpt-124465\n",
      "minibatch training loss: 19.3162\n",
      "minibatch training loss: 18.0895\n",
      "minibatch training loss: 29.5799\n",
      "minibatch training loss: 24.2685\n",
      "minibatch training loss: 24.2367\n",
      "Epoch 32 - train loss: 131785.4062\n",
      "save checkpoint in model_ckpt/model.ckpt-128480\n",
      "minibatch training loss: 23.8463\n",
      "minibatch training loss: 21.6619\n",
      "minibatch training loss: 48.1253\n",
      "minibatch training loss: 24.7931\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "minibatch training loss: 25.4641\n",
      "Epoch 33 - train loss: 257492.2500\n",
      "save checkpoint in model_ckpt/model.ckpt-132495\n",
      "minibatch training loss: 95.5601\n",
      "minibatch training loss: 26.9170\n",
      "minibatch training loss: 18.7067\n",
      "minibatch training loss: 23.1196\n",
      "minibatch training loss: 24.8165\n",
      "Epoch 34 - train loss: 163850.4219\n",
      "save checkpoint in model_ckpt/model.ckpt-136510\n",
      "minibatch training loss: 49.3528\n",
      "minibatch training loss: 27.5813\n",
      "minibatch training loss: 68.2010\n",
      "minibatch training loss: 140.3458\n",
      "minibatch training loss: 40.2573\n",
      "Epoch 35 - train loss: 267039.8125\n",
      "save checkpoint in model_ckpt/model.ckpt-140525\n",
      "minibatch training loss: 102.0956\n",
      "minibatch training loss: 53.5769\n",
      "minibatch training loss: 24.8734\n",
      "minibatch training loss: 54.7208\n",
      "minibatch training loss: 41.2399\n",
      "Epoch 36 - train loss: 421868.1562\n",
      "save checkpoint in model_ckpt/model.ckpt-144540\n",
      "minibatch training loss: 73.9780\n",
      "minibatch training loss: 107.8315\n",
      "minibatch training loss: 926.2928\n",
      "minibatch training loss: 57.4842\n",
      "minibatch training loss: 91.3703\n",
      "Epoch 37 - train loss: 1176527.2500\n",
      "save checkpoint in model_ckpt/model.ckpt-148555\n",
      "minibatch training loss: 59.3735\n",
      "minibatch training loss: 32.4341\n",
      "minibatch training loss: 33.5310\n",
      "minibatch training loss: 130.7366\n",
      "minibatch training loss: 43.4515\n",
      "Epoch 38 - train loss: 915065.0000\n",
      "save checkpoint in model_ckpt/model.ckpt-152570\n",
      "minibatch training loss: 52.9319\n",
      "minibatch training loss: 114.7594\n",
      "minibatch training loss: 517.1043\n",
      "minibatch training loss: 135.5072\n",
      "minibatch training loss: 25.6994\n",
      "Epoch 39 - train loss: 992525.5000\n",
      "save checkpoint in model_ckpt/model.ckpt-156585\n",
      "minibatch training loss: 63.1473\n",
      "minibatch training loss: 28.9836\n",
      "minibatch training loss: 187.2840\n",
      "minibatch training loss: 110.6620\n",
      "minibatch training loss: 36.6051\n",
      "Epoch 40 - train loss: 976399.6250\n",
      "save checkpoint in model_ckpt/model.ckpt-160600\n",
      "minibatch training loss: 89.3469\n",
      "minibatch training loss: 244.7099\n",
      "minibatch training loss: 129.6474\n",
      "minibatch training loss: 141.7676\n",
      "minibatch training loss: 109.2141\n",
      "Epoch 41 - train loss: 1269474.5000\n",
      "save checkpoint in model_ckpt/model.ckpt-164615\n",
      "minibatch training loss: 85.5400\n",
      "minibatch training loss: 115.3419\n",
      "minibatch training loss: 33.6702\n",
      "minibatch training loss: 38.2704\n",
      "minibatch training loss: 67.6237\n",
      "Epoch 42 - train loss: 1060376.6250\n",
      "save checkpoint in model_ckpt/model.ckpt-168630\n",
      "minibatch training loss: 732.5399\n",
      "minibatch training loss: 40.5437\n",
      "minibatch training loss: 1777.7065\n",
      "minibatch training loss: 66.0081\n",
      "minibatch training loss: 59.0389\n",
      "Epoch 43 - train loss: 1014164.5000\n",
      "save checkpoint in model_ckpt/model.ckpt-172645\n",
      "minibatch training loss: 26.9650\n",
      "minibatch training loss: 37.4100\n",
      "minibatch training loss: 26.0142\n",
      "minibatch training loss: 26.2716\n",
      "minibatch training loss: 33.9761\n",
      "Epoch 44 - train loss: 409101.7188\n",
      "save checkpoint in model_ckpt/model.ckpt-176660\n",
      "minibatch training loss: 23.8755\n",
      "minibatch training loss: 24.3763\n",
      "minibatch training loss: 925.5455\n",
      "minibatch training loss: 85.9867\n",
      "minibatch training loss: 43.3607\n",
      "Epoch 45 - train loss: 403159.0000\n",
      "save checkpoint in model_ckpt/model.ckpt-180675\n",
      "minibatch training loss: 37.8673\n",
      "minibatch training loss: 106.0977\n",
      "minibatch training loss: 30.9575\n",
      "minibatch training loss: 81.3040\n",
      "minibatch training loss: 55.6465\n",
      "Epoch 46 - train loss: 1851808.7500\n",
      "save checkpoint in model_ckpt/model.ckpt-184690\n",
      "minibatch training loss: 171.9047\n",
      "minibatch training loss: 288.6021\n",
      "minibatch training loss: 76.6039\n",
      "minibatch training loss: 257.9487\n",
      "minibatch training loss: 186.5757\n",
      "Epoch 47 - train loss: 5364632.0000\n",
      "save checkpoint in model_ckpt/model.ckpt-188705\n",
      "minibatch training loss: 64.0994\n",
      "minibatch training loss: 172.8596\n",
      "minibatch training loss: 196.3679\n",
      "minibatch training loss: 114.9354\n",
      "minibatch training loss: 126.0220\n",
      "Epoch 48 - train loss: 3993958.7500\n",
      "save checkpoint in model_ckpt/model.ckpt-192720\n",
      "minibatch training loss: 145.7128\n",
      "minibatch training loss: 3515.2881\n",
      "minibatch training loss: 485.4449\n",
      "minibatch training loss: 94.1474\n",
      "minibatch training loss: 278.4282\n",
      "Epoch 49 - train loss: 3162776.5000\n",
      "save checkpoint in model_ckpt/model.ckpt-196735\n",
      "minibatch training loss: 2132.3721\n",
      "minibatch training loss: 277.7508\n",
      "minibatch training loss: 542.5215\n",
      "minibatch training loss: 204.0942\n",
      "minibatch training loss: 91.4134\n",
      "Epoch 50 - train loss: 5110187.5000\n",
      "save checkpoint in model_ckpt/model.ckpt-200750\n",
      "Done\n",
      "2018-12-03 08:54:26.853607: Done training.\n"
     ]
    }
   ],
   "source": [
    "from datetime import datetime\n",
    "# start training\n",
    "print(\"{}: Start training.\".format(datetime.now()))\n",
    "model.train( training_filenames, num_train_records)\n",
    "print(\"{}: Done training.\".format(datetime.now()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_captions(model, enc_map, dec_map, img_test, max_len=15):\n",
    "    img_ids, caps = [], []\n",
    "\n",
    "    with get_allow_growth_session() as sess:\n",
    "        saver = tf.train.Saver()\n",
    "        # restore variables from disk.\n",
    "        ckpt = tf.train.get_checkpoint_state(hparams.ckpt_dir)\n",
    "        if ckpt and ckpt.model_checkpoint_path:\n",
    "            saver.restore(sess, tf.train.latest_checkpoint(hparams.ckpt_dir))\n",
    "\n",
    "            counter = 0\n",
    "            total = len(list(img_test.items()))\n",
    "            for img_id, img in img_test.items():\n",
    "                if counter % 500 == 0:\n",
    "                    print('Processing {}/{}'.format(counter, total))\n",
    "                counter += 1\n",
    "                img_ids.append(img_id)\n",
    "                img = np.expand_dims(img, axis=0)\n",
    "                caps.append(model.beam_inference(sess, img, enc_map, dec_map))\n",
    "\n",
    "        else:\n",
    "            print(\"No checkpoint found.\")\n",
    "\n",
    "    return pd.DataFrame({\n",
    "        'img_id': img_ids,\n",
    "        'caption': caps\n",
    "    }).set_index(['img_id'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Shape must be rank 3 but is rank 2 for 'rnn_scope/initial_state' (op: 'ConcatV2') with input shapes: [2,?,256], [?,256], [?,38400], [].",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mInvalidArgumentError\u001b[0m                      Traceback (most recent call last)",
      "\u001b[1;32mc:\\users\\aria\\anaconda3\\envs\\tensorflow_gpuenv\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\u001b[0m in \u001b[0;36m_create_c_op\u001b[1;34m(graph, node_def, inputs, control_inputs)\u001b[0m\n\u001b[0;32m   1627\u001b[0m   \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1628\u001b[1;33m     \u001b[0mc_op\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mc_api\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTF_FinishOperation\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mop_desc\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1629\u001b[0m   \u001b[1;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mInvalidArgumentError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mInvalidArgumentError\u001b[0m: Shape must be rank 3 but is rank 2 for 'rnn_scope/initial_state' (op: 'ConcatV2') with input shapes: [2,?,256], [?,256], [?,38400], [].",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-103-caf0ba6e7f9d>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreset_default_graph\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mmodel\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mImageCaptionModel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mhparams\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'inference'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 5\u001b[1;33m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbuild\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      6\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[1;31m# load test image  size=20548\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-85-f97a460b2fa8>\u001b[0m in \u001b[0;36mbuild\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    169\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_build_inputs\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    170\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_build_seq_embeddings\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 171\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_build_model\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    172\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    173\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mtrain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtraining_filenames\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnum_train_records\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-85-f97a460b2fa8>\u001b[0m in \u001b[0;36m_build_model\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     95\u001b[0m                 \u001b[1;31m#  use concatenated states for convenient feeding and fetching.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     96\u001b[0m                 initial_state = tf.concat(\n\u001b[1;32m---> 97\u001b[1;33m                     values=initial_state, axis=1, name='initial_state')\n\u001b[0m\u001b[0;32m     98\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     99\u001b[0m                 state_feed = tf.placeholder(\n",
      "\u001b[1;32mc:\\users\\aria\\anaconda3\\envs\\tensorflow_gpuenv\\lib\\site-packages\\tensorflow\\python\\ops\\array_ops.py\u001b[0m in \u001b[0;36mconcat\u001b[1;34m(values, axis, name)\u001b[0m\n\u001b[0;32m   1122\u001b[0m               tensor_shape.scalar())\n\u001b[0;32m   1123\u001b[0m       \u001b[1;32mreturn\u001b[0m \u001b[0midentity\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mscope\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1124\u001b[1;33m   \u001b[1;32mreturn\u001b[0m \u001b[0mgen_array_ops\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconcat_v2\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0maxis\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1125\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1126\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\aria\\anaconda3\\envs\\tensorflow_gpuenv\\lib\\site-packages\\tensorflow\\python\\ops\\gen_array_ops.py\u001b[0m in \u001b[0;36mconcat_v2\u001b[1;34m(values, axis, name)\u001b[0m\n\u001b[0;32m   1031\u001b[0m     \u001b[0m_attr_N\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1032\u001b[0m     _, _, _op = _op_def_lib._apply_op_helper(\n\u001b[1;32m-> 1033\u001b[1;33m         \"ConcatV2\", values=values, axis=axis, name=name)\n\u001b[0m\u001b[0;32m   1034\u001b[0m     \u001b[0m_result\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_op\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1035\u001b[0m     \u001b[0m_inputs_flat\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_op\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\aria\\anaconda3\\envs\\tensorflow_gpuenv\\lib\\site-packages\\tensorflow\\python\\framework\\op_def_library.py\u001b[0m in \u001b[0;36m_apply_op_helper\u001b[1;34m(self, op_type_name, name, **keywords)\u001b[0m\n\u001b[0;32m    785\u001b[0m         op = g.create_op(op_type_name, inputs, output_types, name=scope,\n\u001b[0;32m    786\u001b[0m                          \u001b[0minput_types\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0minput_types\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mattrs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mattr_protos\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 787\u001b[1;33m                          op_def=op_def)\n\u001b[0m\u001b[0;32m    788\u001b[0m       \u001b[1;32mreturn\u001b[0m \u001b[0moutput_structure\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mop_def\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mis_stateful\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mop\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    789\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\aria\\anaconda3\\envs\\tensorflow_gpuenv\\lib\\site-packages\\tensorflow\\python\\util\\deprecation.py\u001b[0m in \u001b[0;36mnew_func\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    486\u001b[0m                 \u001b[1;34m'in a future version'\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mdate\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m \u001b[1;32melse\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;34m'after %s'\u001b[0m \u001b[1;33m%\u001b[0m \u001b[0mdate\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    487\u001b[0m                 instructions)\n\u001b[1;32m--> 488\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    489\u001b[0m     return tf_decorator.make_decorator(func, new_func, 'deprecated',\n\u001b[0;32m    490\u001b[0m                                        _add_deprecated_arg_notice_to_docstring(\n",
      "\u001b[1;32mc:\\users\\aria\\anaconda3\\envs\\tensorflow_gpuenv\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\u001b[0m in \u001b[0;36mcreate_op\u001b[1;34m(***failed resolving arguments***)\u001b[0m\n\u001b[0;32m   3272\u001b[0m           \u001b[0minput_types\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0minput_types\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3273\u001b[0m           \u001b[0moriginal_op\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_default_original_op\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 3274\u001b[1;33m           op_def=op_def)\n\u001b[0m\u001b[0;32m   3275\u001b[0m       \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_create_op_helper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mret\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcompute_device\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcompute_device\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3276\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mret\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\aria\\anaconda3\\envs\\tensorflow_gpuenv\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, node_def, g, inputs, output_types, control_inputs, input_types, original_op, op_def)\u001b[0m\n\u001b[0;32m   1790\u001b[0m           op_def, inputs, node_def.attr)\n\u001b[0;32m   1791\u001b[0m       self._c_op = _create_c_op(self._graph, node_def, grouped_inputs,\n\u001b[1;32m-> 1792\u001b[1;33m                                 control_input_ops)\n\u001b[0m\u001b[0;32m   1793\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1794\u001b[0m     \u001b[1;31m# Initialize self._outputs.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\aria\\anaconda3\\envs\\tensorflow_gpuenv\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\u001b[0m in \u001b[0;36m_create_c_op\u001b[1;34m(graph, node_def, inputs, control_inputs)\u001b[0m\n\u001b[0;32m   1629\u001b[0m   \u001b[1;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mInvalidArgumentError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1630\u001b[0m     \u001b[1;31m# Convert to ValueError for backwards compatibility.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1631\u001b[1;33m     \u001b[1;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1632\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1633\u001b[0m   \u001b[1;32mreturn\u001b[0m \u001b[0mc_op\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: Shape must be rank 3 but is rank 2 for 'rnn_scope/initial_state' (op: 'ConcatV2') with input shapes: [2,?,256], [?,256], [?,38400], []."
     ]
    }
   ],
   "source": [
    "from datetime import datetime\n",
    "# create model\n",
    "tf.reset_default_graph()\n",
    "model = ImageCaptionModel(hparams, mode='inference')\n",
    "model.build()\n",
    "\n",
    "# load test image  size=20548\n",
    "img_test = cPickle.load(open('dataset/test_img256.pkl', 'rb'))\n",
    "\n",
    "# generate caption to csv file\n",
    "print(\"{}: Start testing.\".format(datetime.now()))\n",
    "%time df_predict = generate_captions(model, enc_map, dec_map, img_test)\n",
    "print(\"{}: Done testing.\".format(datetime.now()))\n",
    "df_predict.to_csv('generated/sample_470_epoch_beam.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.system('cd dataset/CIDErD_win  gen_score.exe -i ../../generated/sample_470_epoch_beam.csv -r ../../generated/score.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
